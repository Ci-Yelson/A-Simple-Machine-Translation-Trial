## ðŸŽ‰A Simple Machine Translation Trial

åŸºäºŽç»å…¸çš„Seq2Seqç½‘ç»œæž¶æž„ï¼Œä½¿ç”¨RNNã€Attentionæœºåˆ¶ã€Transformerç­‰ç½‘ç»œæˆ–æŠ€æœ¯å®žçŽ°ä¸€ä¸ªç®€å•çš„English-to-Chineseæœºå™¨ç¿»è¯‘ä»»åŠ¡ã€‚

### ðŸ“–Data Source

http://www.manythings.org/anki/

Data sample:

```
hi .	å—¨ ã€‚
hi .	ä½  å¥½ ã€‚
run .	ä½  ç”¨ è·‘ çš„ ã€‚
stop !	ä½ æ‰‹ ï¼
wait !	ç­‰ ç­‰ ï¼
wait !	ç­‰ ä¸€ ä¸‹ ï¼
```

Data rows: `29155`

#### Data Preprocess

è‹±æ–‡åˆ†è¯ï¼šä½¿ç”¨word-level Tokenizeã€‚

ä¸­æ–‡åˆ†è¯ï¼šä½¿ç”¨Jiebaåˆ†è¯åº“å®žçŽ°word-level Tokenizeã€‚

åˆ†è¯æ•ˆæžœï¼š

![image-01](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/data_sample.png)

ä¸ºå‡å°è¯æ±‡è¡¨å¤§å°ï¼Œåœ¨å®žçŽ°ä¸­å¯¹å‡ºçŽ°æ¬¡æ•°å°äºŽ2æ¬¡çš„è¯å…ƒè¿›è¡Œäº†ç§»é™¤ã€‚

åˆ†è¯åŽç”Ÿæˆçš„è¯æ±‡è¡¨å¤§å°ï¼š`eng_vocab_size: 4486, cmn_vocab_size: 6515`ã€‚

### ðŸŽModels

ä½¿ç”¨é€šç”¨çš„â€œç¼–ç å™¨-è§£ç å™¨â€Seq2Seqç½‘ç»œæž¶æž„ã€‚

#### Baseline RNN

ä½¿ç”¨RNNä½œä¸ºBaselineï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨å‡ä½¿ç”¨GRU Cellå®žçŽ°ã€‚

#### Add Attention Mechanism

åœ¨åŽŸRNN Encoder-Decoderçš„åŸºç¡€ä¸ŠåŠ å…¥Additive Attention Mechanismã€‚

$$
Attention(Q,K,V) = \text{dropout}\bigg(\operatorname{softmax}\left(\mathbf{W}_{v} \tanh \left(\mathbf{W}_{q} \mathbf{Q}+\mathbf{W}_{k} \mathbf{K}\right)\right)\bigg) \mathbf{V}
$$

åœ¨Decoderçš„æ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼ŒDecoderä¸Šä¸€æ—¶é—´æ­¥çš„æœ€ç»ˆå±‚éšçŠ¶æ€ä½œä¸ºæŸ¥è¯¢Qï¼ŒEncoderåœ¨æ‰€æœ‰æ—¶é—´æ­¥çš„æœ€ç»ˆå±‚éšçŠ¶æ€ä½œä¸ºé”®Kå’Œå€¼Vã€‚

#### Transformer

ä½¿ç”¨äº†åŸºäºŽå…¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Scaled Dot-Product Attentionï¼‰çš„Transformeræ¨¡åž‹ã€‚

> åœ¨å®žçŽ°ä¸­ï¼ŒRNNæ¨¡åž‹çš„`embed_size=512, hidden_size=1024, num_layers=2`ï¼›Transformeræ¨¡åž‹çš„`hidden_size=512, num_heads=4, num_layers=2`ã€‚
>
> æ›´å…·ä½“çš„å‚æ•°è®¾ç½®å‚è§ä»£ç å®žçŽ°ã€‚

### ðŸ”–Training Results

#### Training loss:

![image-02](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/loss.png)

å…¶ä¸­ï¼ŒbidRNNä¸ºåœ¨ç¼–ç å™¨éƒ¨åˆ†ä½¿ç”¨äº†åŒå‘ç»“æž„çš„RNNã€‚

#### BLEU Scores

ä½¿ç”¨BLEU Scoreså¯¹æ¨¡åž‹çš„ç¿»è¯‘æ•ˆæžœè¿›è¡Œè¯„ä¼°ã€‚

![image-03](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/metrics.png)



å¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼š

- åœ¨è®­ç»ƒlossæ–¹é¢ï¼Œç”±äºŽtransformerç›¸å¯¹RNNç¼ºå°‘å¯¹åºåˆ—ä¿¡æ¯çš„å‡è®¾ï¼Œå› æ­¤åœ¨è®­ç»ƒçš„æ”¶æ•›ç¨‹åº¦ä¸Šï¼Œtransformerä½ŽäºŽRNNã€‚
- åœ¨BLEU Scoresæ–¹é¢ï¼Œå¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡åž‹çš„å¾—åˆ†è¡¨çŽ°æ¯”æ²¡æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„è¦å¥½å¾ˆå¤šã€‚åŒæ—¶æ³¨æ„åˆ°ï¼Œè™½ç„¶tansformerç›¸å¯¹RNNæ”¶æ•›ç¨‹åº¦è¾ƒå·®ï¼Œä½†å…¶BLEUå¾—åˆ†å´æ˜¯æœ€é«˜çš„ã€‚
- åœ¨è®­ç»ƒè€—æ—¶æ–¹é¢ï¼Œå¯ä»¥å‘çŽ°ï¼Œç”±äºŽtransformerçš„å¹¶è¡ŒåŒ–ä¼˜åŠ¿ï¼Œå…¶è®­ç»ƒè€—æ—¶ç›¸å¯¹RNNä¼šçŸ­å¾ˆå¤šã€‚



### ðŸ…°ï¸Attention Weights Visualize

#### RNN with Attention Mechanism

![image-04](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/attention_sample.png)

#### Transformer Attention

![image-05](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/transformer_enc_attention.png)

![image-06](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/transformer_dec_self_attention.png)

![image-07](https://github.com/Ci-Yelson/A-Simple-Machine-Translation-Trial/blob/main/img/transformer_dec_inter_attention.png)

### ðŸ“‘References

> - [ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹](https://zh-v2.d2l.ai)
> - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
> - [CS231n Assignment 3](https://cs231n.github.io/assignments2022/assignment3/)
