{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:19:43.922443Z","iopub.status.busy":"2022-09-06T10:19:43.922061Z","iopub.status.idle":"2022-09-06T10:19:43.930602Z","shell.execute_reply":"2022-09-06T10:19:43.929520Z","shell.execute_reply.started":"2022-09-06T10:19:43.922410Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","\n","import os\n","import time\n","import json\n","import munch\n","\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import seaborn as sns\n","\n","plt.style.use('seaborn')\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","plt.rcParams['font.sans-serif'] = ['SimHei']\n","plt.rcParams['axes.unicode_minus']=False"]},{"cell_type":"markdown","metadata":{},"source":["### 0x01超参数设置"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:33.443440Z","iopub.status.busy":"2022-09-06T09:17:33.440430Z","iopub.status.idle":"2022-09-06T09:17:33.457940Z","shell.execute_reply":"2022-09-06T09:17:33.456757Z","shell.execute_reply.started":"2022-09-06T09:17:33.443394Z"},"trusted":true},"outputs":[],"source":["cfg = munch.Munch({\n","    'eng_vocab_path': './dataset/cmneng/eng_vocab_m2.json',\n","    'cmn_vocab_path': './dataset/cmneng/cmn_vocab_m2_jb.json',\n","    'data_path': './dataset/cmneng/data_jb.txt',\n","    \n","    # model save\n","    'enc_model_path': './enc',\n","    'dec_model_path': './dec',\n","    \n","    # network config\n","    'embed_size': 512,\n","    'hidden_size': 1024,\n","    'num_layers': 2,\n","    'dropout': 0.1,\n","    'batch_size': 128,\n","    'lr': 0.001,\n","    'num_epoches': 20,\n","    \n","    # transformer\n","    'transformer_hidden_size': 512,\n","    'num_heads': 4,\n","    \n","    # for plot\n","    'all_losslog': dict(),\n","})\n","\n","cfg.UNK = 0\n","cfg.PAD = 1\n","cfg.START = 2\n","cfg.END = 3\n","\n","print(cfg)"]},{"cell_type":"markdown","metadata":{},"source":["#### Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:33.465750Z","iopub.status.busy":"2022-09-06T09:17:33.462971Z","iopub.status.idle":"2022-09-06T09:17:33.480177Z","shell.execute_reply":"2022-09-06T09:17:33.479215Z","shell.execute_reply.started":"2022-09-06T09:17:33.465709Z"},"trusted":true},"outputs":[],"source":["import re\n","from matplotlib_inline import backend_inline\n","\n","\n","def eng_transform(data):\n","    data = re.sub(r'([\\.\\?\\!\\,\\\"])', r\" \\1 \", data)  # 在特殊字符前后加空格\n","    data = re.sub(r'[^a-zA-Z\\.\\?\\!\\,\\\"\\']', r\" \", data).strip().lower()\n","    data = re.sub(r' +', r\" \", data)  # 将多个空格替换为单个空格\n","    return data\n","\n","def show_heatmaps(matries, xlabel, ylabel, titles=None, suptitle=None, figsize=(2.5, 2.5), cmap='Reds'):\n","    backend_inline.set_matplotlib_formats('svg')\n","\n","    num_rows, num_cols = matries.shape[0], matries.shape[1]\n","    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize,\n","                            sharex=True, sharey=True, squeeze=False)\n","    for i, (row_axes, row_matrices) in enumerate(zip(axes, matries)):\n","        for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):\n","            pcm = ax.imshow(matrix, cmap=cmap)\n","            if i == num_rows - 1:\n","                ax.set_xlabel(xlabel)\n","            if j == 0:\n","                ax.set_ylabel(ylabel)\n","            if titles:\n","                ax.set_title(titles[j])\n","    if suptitle:\n","        fig.suptitle(suptitle)\n","    fig.colorbar(pcm, ax=axes, shrink=0.6)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 0x02准备数据"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:33.489501Z","iopub.status.busy":"2022-09-06T09:17:33.487025Z","iopub.status.idle":"2022-09-06T09:17:33.529330Z","shell.execute_reply":"2022-09-06T09:17:33.528434Z","shell.execute_reply.started":"2022-09-06T09:17:33.489445Z"},"trusted":true},"outputs":[],"source":["eng_vocab = dict()\n","with open(cfg.eng_vocab_path, 'r', encoding='utf-8') as f:\n","    eng_vocab_dict = json.load(f)\n","    for k, v in eng_vocab_dict.items():\n","        eng_vocab[k] = v\n","\n","cmn_vocab = dict()\n","with open(cfg.cmn_vocab_path, 'r', encoding='utf-8') as f:\n","    cmn_vocab_dict = json.load(f)\n","    for k, v in cmn_vocab_dict.items():\n","        cmn_vocab[k] = v\n","\n","cfg.eng_vocab = eng_vocab\n","cfg.cmn_vocab = cmn_vocab\n","\n","cfg.eng_vocab_size = len(eng_vocab['token_to_idx'])\n","cfg.cmn_vocab_size = len(cmn_vocab['token_to_idx'])\n","\n","print(list(eng_vocab['token_to_idx'].items())[:10])\n","print(list(cmn_vocab['token_to_idx'].items())[:10])\n","\n","print(f'eng_vocab_size: {cfg.eng_vocab_size}, cmn_vocab_size: {cfg.cmn_vocab_size}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:33.536169Z","iopub.status.busy":"2022-09-06T09:17:33.533901Z","iopub.status.idle":"2022-09-06T09:17:33.728220Z","shell.execute_reply":"2022-09-06T09:17:33.727045Z","shell.execute_reply.started":"2022-09-06T09:17:33.536119Z"},"trusted":true},"outputs":[],"source":["org_data = pd.read_csv(cfg.data_path, sep='\\t', header=None, quoting=3)\n","org_data.sample(frac=1).head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:33.736168Z","iopub.status.busy":"2022-09-06T09:17:33.733468Z","iopub.status.idle":"2022-09-06T09:17:34.047868Z","shell.execute_reply":"2022-09-06T09:17:34.046641Z","shell.execute_reply.started":"2022-09-06T09:17:33.736124Z"},"trusted":true},"outputs":[],"source":["org_data[0] = org_data[0].apply(lambda x: x.split())\n","org_data[1] = org_data[1].apply(lambda x: x.split())\n","org_data.sample(frac=1).head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:34.052383Z","iopub.status.busy":"2022-09-06T09:17:34.051998Z","iopub.status.idle":"2022-09-06T09:17:34.114044Z","shell.execute_reply":"2022-09-06T09:17:34.112945Z","shell.execute_reply.started":"2022-09-06T09:17:34.052342Z"},"trusted":true},"outputs":[],"source":["# 平均长度\n","eng_len = org_data[0].apply(lambda x: len(x)).mean()\n","cmn_len = org_data[1].apply(lambda x: len(x)).mean()\n","print(f'eng_avg_len: {eng_len}, cmnavg_len: {cmn_len}')\n","\n","cfg.seq_len = int(20)"]},{"cell_type":"markdown","metadata":{},"source":["#### 将数据转换为向量表示"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:34.118274Z","iopub.status.busy":"2022-09-06T09:17:34.117915Z","iopub.status.idle":"2022-09-06T09:17:35.230455Z","shell.execute_reply":"2022-09-06T09:17:35.229497Z","shell.execute_reply.started":"2022-09-06T09:17:34.118244Z"},"trusted":true},"outputs":[],"source":["def line_to_idx(line, lang):\n","    if len(line) > cfg.seq_len-1:\n","        line = line[:cfg.seq_len-1]  # 截断\n","    line = line + ['<END>']\n","    line = line + ['<PAD>'] * (cfg.seq_len - len(line))\n","    \n","    eng_unk = eng_vocab['token_to_idx'].get('<UNK>', 0)\n","    cmn_unk = cmn_vocab['token_to_idx'].get('<UNK>', 0)\n","    \n","    if lang == 'eng':\n","        return [eng_vocab['token_to_idx'].get(t, eng_unk) for t in line]\n","    if lang == 'cmn':\n","        return [cmn_vocab['token_to_idx'].get(t, cmn_unk) for t in line]\n","    \n","data = org_data.copy()\n","data[0] = org_data[0].apply(lambda x: line_to_idx(x, 'eng'))\n","data[1] = org_data[1].apply(lambda x: line_to_idx(x, 'cmn'))\n","data.sample(frac=1).head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.233616Z","iopub.status.busy":"2022-09-06T09:17:35.232320Z","iopub.status.idle":"2022-09-06T09:17:35.259602Z","shell.execute_reply":"2022-09-06T09:17:35.258511Z","shell.execute_reply.started":"2022-09-06T09:17:35.233577Z"},"trusted":true},"outputs":[],"source":["# random_shuffle \n","np.random.seed(443)\n","cfg.data = data.sample(frac=1)\n","print(cfg.data.shape)\n","cfg.data.head()"]},{"cell_type":"markdown","metadata":{},"source":["#### 自定义Dataset和DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.265916Z","iopub.status.busy":"2022-09-06T09:17:35.265447Z","iopub.status.idle":"2022-09-06T09:17:35.436643Z","shell.execute_reply":"2022-09-06T09:17:35.435504Z","shell.execute_reply.started":"2022-09-06T09:17:35.265874Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","\n","class TextDataset(Dataset):\n","    def __init__(self, mode=\"train\", valid_ratio=0.1, test_ratio=0.1):\n","        self.df = cfg.data\n","        self.train_len = int(len(self.df) * (1 - valid_ratio - test_ratio))\n","        self.valid_len = int(len(self.df) * valid_ratio)\n","        self.test_len = int(len(self.df) * test_ratio)\n","        \n","        if mode == 'train':\n","            self.eng_data = torch.tensor(self.df[:self.train_len][0].to_list())\n","            self.cmn_data = torch.tensor(self.df[:self.train_len][1].to_list())\n","        elif mode == 'valid':\n","            self.eng_data = torch.tensor(self.df[self.train_len:self.train_len + self.valid_len][0].to_list())\n","            self.cmn_data = torch.tensor(self.df[self.train_len:self.train_len + self.valid_len][1].to_list())\n","        elif mode == 'test':\n","            self.eng_data = torch.tensor(self.df[self.train_len + self.valid_len:][0].to_list())\n","            self.cmn_data = torch.tensor(self.df[self.train_len + self.valid_len:][1].to_list())\n","        \n","        print(f\"Finish loading {mode} data ({len(self.eng_data)} samples)\")\n","        \n","    def __len__(self):\n","        return len(self.eng_data)\n","    \n","    def __getitem__(self, idx):\n","        return self.eng_data[idx], self.cmn_data[idx]\n","    \n","    \n","def get_dataloader():\n","    train_dataset = TextDataset(mode='train')\n","    valid_dataset = TextDataset(mode='valid')\n","    test_dataset = TextDataset(mode='test')\n","    \n","    train_dataloader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n","    valid_dataloader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=True)\n","    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n","    \n","    return train_dataloader, valid_dataloader, test_dataloader\n","\n","cfg.train_dataloader, cfg.valid_dataloader, cfg.test_dataloader = get_dataloader()\n","\n","\n","def eng_cmn_to_text(tensor_tuple):\n","    return [cfg.eng_vocab['idx_to_token'][t] for t in tensor_tuple[0]], [cfg.cmn_vocab['idx_to_token'][t] for t in tensor_tuple[1]]\n","\n","print(f'\\nSample data:')\n","print(cfg.train_dataloader.dataset[0])\n","print(eng_cmn_to_text(cfg.train_dataloader.dataset[0]))"]},{"cell_type":"markdown","metadata":{},"source":["### 0x03定义Seq2Seq-RNN网络模型"]},{"cell_type":"markdown","metadata":{},"source":["#### Seq2SeqEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.438756Z","iopub.status.busy":"2022-09-06T09:17:35.438001Z","iopub.status.idle":"2022-09-06T09:17:35.581799Z","shell.execute_reply":"2022-09-06T09:17:35.580563Z","shell.execute_reply.started":"2022-09-06T09:17:35.438715Z"},"trusted":true},"outputs":[],"source":["class Seq2SeqEncoder(nn.Module):\n","    ''' 编码器 '''\n","    def __init__(self,vocab_size, embed_size, hidden_size, num_layers, dropout=0.1, withBidirectional=False):\n","        super(Seq2SeqEncoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.rnn = nn.GRU(embed_size, hidden_size, num_layers, dropout=dropout, bidirectional=withBidirectional)\n","        self.withBidirectional = withBidirectional\n","        if withBidirectional is True:\n","            self.fc1 = nn.Linear(2*hidden_size, hidden_size)\n","            self.fc2 = nn.Linear(2*hidden_size, hidden_size)\n","        \n","    def forward(self, x):\n","        '''\n","        x: (batch_size, seq_len)\n","        '''\n","        x = self.embedding(x) # (batch_size, seq_len, embed_size)\n","        x = x.permute(1, 0, 2) # (seq_len, batch_size, embed_size)\n","        output, state = self.rnn(x)\n","        \n","        if self.withBidirectional is True:\n","            output = self.fc1(output) # only the attention_decoder use the enc_output\n","            \n","            num_layers = state.shape[0]//2\n","                \n","            state = torch.concat((state[:num_layers], state[num_layers:]), dim=-1)\n","            state = self.fc2(state)\n","            state = torch.tanh(state)\n","            \n","        # output: (seq_len, batch_size, hidden_size)\n","        # state: (num_layers, batch_size, hidden_size)\n","        return output, state\n","    \n","    \n","encoder_tester = Seq2SeqEncoder(vocab_size=1024, embed_size=128, hidden_size=128, num_layers=2, dropout=0.1, withBidirectional=True)\n","encoder_tester.eval()\n","x_test = torch.ones((64, 10), dtype=torch.long)\n","output, state = encoder_tester(x_test)\n","output.shape, state.shape\n","# (seq_len, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n","# if withBidirectional: (seq_len, batch_size, hidden_size), (2*num_layers, batch_size, hidden_size)"]},{"cell_type":"markdown","metadata":{},"source":["#### Seq2SeqDecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.593127Z","iopub.status.busy":"2022-09-06T09:17:35.588596Z","iopub.status.idle":"2022-09-06T09:17:35.748777Z","shell.execute_reply":"2022-09-06T09:17:35.747543Z","shell.execute_reply.started":"2022-09-06T09:17:35.593054Z"},"trusted":true},"outputs":[],"source":["class Seq2SeqDecoder(nn.Module):\n","    ''' 解码器 '''\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.1):\n","        super(Seq2SeqDecoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        # 为了进一步包含经过编码的输入序列的信息， 上下文变量context在所有的时间步与解码器的输入dec_input进行拼接（concatenate）。\n","        self.rnn = nn.GRU(embed_size+hidden_size, hidden_size, num_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","    def init_state(self, enc_outputs, enc_valid_len):\n","        '''\n","        enc_outputs: ((seq_len, batch_size, hidden_size), (num_layers, batch_size, hidden_size))\n","        '''\n","        return enc_outputs[1]\n","    \n","    def forward(self, x, state):\n","        '''\n","        x: (batch_size, seq_len)\n","        state: (num_layers, batch_size, hidden_size)\n","        '''\n","        x = self.embedding(x).permute(1, 0, 2) # (seq_len, batch_size, embed_size)\n","        context = state[-1].repeat(x.shape[0], 1, 1) # (seq_len, batch_size, hidden_size)\n","        x_context = torch.cat([x, context], dim=-1) # (seq_len, batch_size, embed_size+hidden_size)\n","        output, state = self.rnn(x_context, state)\n","        # output: (seq_len, batch_size, hidden_size)\n","        # state: (num_layers, batch_size, hidden_size)\n","        output = self.fc(output).permute(1, 0, 2)\n","        # output: (batch_size, seq_len, vocab_size)\n","        return output, state\n","    \n","decoder_tester = Seq2SeqDecoder(vocab_size=1024, embed_size=128, hidden_size=128, num_layers=2, dropout=0.1)\n","x_test = torch.ones((64, 10), dtype=torch.long)\n","x_valid_len = torch.sum(x_test!=cfg.PAD, dim=-1)\n","state = decoder_tester.init_state(encoder_tester(x_test), x_valid_len)\n","output, state = decoder_tester(x_test, state)\n","output.shape, state.shape\n","# (batch_size, seq_len, vocab_size), (num_layers, batch_size, hidden_size)"]},{"cell_type":"markdown","metadata":{},"source":["### 0x04定义Seq2SeqSolver"]},{"cell_type":"markdown","metadata":{},"source":["#### MaskedSoftmaxCELoss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.751265Z","iopub.status.busy":"2022-09-06T09:17:35.750491Z","iopub.status.idle":"2022-09-06T09:17:35.802875Z","shell.execute_reply":"2022-09-06T09:17:35.801461Z","shell.execute_reply.started":"2022-09-06T09:17:35.751222Z"},"trusted":true},"outputs":[],"source":["cfg.PAD = 1\n","\n","class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n","    def get_mask(self, target, valid_len):\n","        mask = torch.ones_like(target)\n","        bool_mask = torch.arange((target.shape[1]), dtype=torch.float32, device=target.device)[None, :] < valid_len[:, None]\n","        mask[~bool_mask] = 0\n","        return mask\n","    \n","    def forward(self, logits, target, valid_len):\n","        '''\n","        logits: (batch_size, seq_len, vocab_size)\n","        target: (batch_size, seq_len)\n","        valid_len: (batch_size,)\n","        '''\n","        mask = self.get_mask(target, valid_len)\n","        self.reduction = 'none'\n","        loss_input = logits.permute(0, 2, 1) # (batch_size, vocab_size, seq_len) [CELoss: input(N,C,d1,d2,...), target(N,d1,d2,...)]\n","        org_loss = super(MaskedSoftmaxCELoss, self).forward(loss_input, target)\n","        loss = (org_loss * mask).mean(dim=1)\n","        return loss\n","        \n","\n","loss = MaskedSoftmaxCELoss()\n","l_input = torch.ones((4, 10, 1024))\n","l_target = torch.ones((4, 10), dtype=torch.long)\n","l_valid_len = torch.tensor([10, 7, 5, 3])\n","print(l_valid_len)\n","\n","mask = loss.get_mask(l_target, l_valid_len)\n","print(mask)\n","\n","loss(l_input, l_target, l_valid_len)"]},{"cell_type":"markdown","metadata":{},"source":["#### BLEU\n","$$\n","\\exp \\left(\\min \\left(0,1-\\frac{\\operatorname{len}_{\\text {label }}}{\\operatorname{len}_{\\text {pred }}}\\right)\\right) \\prod_{n=1}^{k} p_{n}^{1 / 2^{n}}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.812800Z","iopub.status.busy":"2022-09-06T09:17:35.810042Z","iopub.status.idle":"2022-09-06T09:17:35.840363Z","shell.execute_reply":"2022-09-06T09:17:35.839223Z","shell.execute_reply.started":"2022-09-06T09:17:35.812754Z"},"trusted":true},"outputs":[],"source":["import math\n","import collections\n","\n","def bleu(orig_seq, pred_seq, k):\n","    orig_tokens, pred_tokens = orig_seq.split(' '), pred_seq.split(' ')\n","    while orig_tokens[-1]=='<PAD>' or orig_tokens[-1]=='<END>':\n","        orig_tokens.pop()\n","    while pred_tokens[-1]=='<PAD>' or pred_tokens[-1]=='<END>':\n","        pred_tokens.pop()\n","    len_orig, len_pred = len(orig_tokens), len(pred_tokens)\n","    \n","    score = math.exp(min(0, 1 - len_orig / len_pred)) # 惩罚项\n","    for n in range(1, k + 1):\n","        match_cnt, orig_subs = 0, collections.defaultdict(int)\n","        for i in range(len_orig - n + 1):\n","            orig_subs[' '.join(orig_tokens[i: i + n])] += 1\n","        for i in range(len_pred - n + 1):\n","            pred_sent = ' '.join(pred_tokens[i: i + n])\n","            if orig_subs[pred_sent] > 0:\n","                match_cnt += 1\n","                orig_subs[pred_sent] -= 1\n","        if len_orig - n + 1 <= 0:\n","            return 0\n","        else:\n","            score *= math.pow(match_cnt / (len_orig - n + 1), math.pow(0.5, n))\n","\n","    return score"]},{"cell_type":"markdown","metadata":{},"source":["#### Seq2SeqSolver"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:35.852764Z","iopub.status.busy":"2022-09-06T09:17:35.847668Z","iopub.status.idle":"2022-09-06T09:17:36.092070Z","shell.execute_reply":"2022-09-06T09:17:36.090471Z","shell.execute_reply.started":"2022-09-06T09:17:35.852723Z"},"trusted":true},"outputs":[],"source":["cfg.PAD = 1\n","cfg.START = 2\n","cfg.END = 3\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class Seq2SeqSolver(nn.Module):\n","    '''\n","    统一的Seq2Seq框架\n","    '''\n","    def __init__(self, encoder, decoder, with_attn=False, is_transformer=False, save_path=''):\n","        super(Seq2SeqSolver, self).__init__()\n","        self.encoder, self.decoder = encoder.to(device), decoder.to(device)\n","        self.seq_len = cfg.seq_len\n","        \n","        self.loss = MaskedSoftmaxCELoss()\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=cfg.lr)\n","        \n","        self.is_transformer = is_transformer\n","        self.with_attn = with_attn\n","        self.attn_weights = None\n","        self.losslog = None\n","        self.save_path = save_path\n","        \n","    def grad_clipping(self, theta):\n","        '''梯度裁剪'''\n","        enc_params = [p for p in self.encoder.parameters() if p.requires_grad and p.grad is not None]\n","        dec_params = [p for p in self.decoder.parameters() if p.requires_grad and p.grad is not None]\n","        params = enc_params + dec_params     \n","        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n","        if norm > theta:\n","            for param in params:\n","                param.grad[:] *= theta / norm\n","        \n","    def train(self, mini_train=False):\n","        self.encoder.train()\n","        self.decoder.train()\n","        self.losslog = []\n","        \n","        num_epoches = cfg.num_epoches if not mini_train else 1\n","        mini_cnt = 0 \n","        for epoch in range(num_epoches):\n","            loss_sum = 0\n","            num_tokens_sum = 0\n","            \n","            for i, (x, y) in enumerate(cfg.train_dataloader):\n","                x, y = x.to(device), y.to(device)\n","                x_valid_len = torch.sum(x!=cfg.PAD, dim=-1)\n","                y_valid_len = torch.sum(y!=cfg.PAD, dim=-1)\n","                \n","                self.optimizer.zero_grad()\n","\n","                ### enc-dec part\n","                if self.is_transformer is True:\n","                    state = self.encoder(x, x_valid_len)\n","                else:\n","                    state = self.encoder(x)\n","                init_state = self.decoder.init_state(state, x_valid_len)\n","                # teacher forcing\n","                start = torch.tensor([cfg.START]*y.shape[0], device=y.device).reshape(-1,1)\n","                dec_input = torch.cat([start, y[:,:-1]], dim=1)\n","                dec_outputs, _ = self.decoder(dec_input, init_state)\n","                ### enc-dec part\n","                \n","                loss = self.loss(dec_outputs, y, y_valid_len)\n","                loss.sum().backward()\n","                self.grad_clipping(1)   # 梯度裁剪\n","                self.optimizer.step()\n","                \n","                num_tokens = y_valid_len.sum()\n","                \n","                with torch.no_grad():\n","                    loss_sum += loss.sum().item()\n","                    num_tokens_sum += num_tokens\n","\n","                # for test only\n","                if mini_train is True:\n","                    if mini_cnt < 10:\n","                        mini_cnt += 1\n","                        print(f'epoch {epoch+1}, batch {i+1}, loss {loss_sum/num_tokens_sum:.4f}')\n","                    else:\n","                        break\n","\n","            print(f'epoch: {epoch}, loss: {loss_sum/num_tokens_sum}')\n","            self.losslog.append(loss_sum/num_tokens_sum)\n","        self.losslog = np.array([x.cpu().numpy() for x in self.losslog])\n","        torch.save(self.encoder.state_dict(), cfg.enc_model_path + '-' + self.save_path + '.pth')\n","        torch.save(self.decoder.state_dict(), cfg.dec_model_path + '-' + self.save_path + '.pth')\n","        \n","                    \n","    def predict(self, en_input, enc_valid_lens):\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        \n","        outputs, attn_weights = [], []\n","        if self.is_transformer is True:\n","            attn_weights = [[], [], []]\n","            state = self.encoder(en_input, enc_valid_lens)\n","            attn_weights[0] = self.encoder.attn_weights\n","            attn_weights[0] = torch.cat(attn_weights[0], dim=0).reshape(cfg.num_layers, cfg.num_heads, -1, cfg.seq_len)\n","            # attn_weights[0]: (num_layers, num_heads, seq_len, seq_len)\n","        else:\n","            state = self.encoder(en_input)\n","        hidden = self.decoder.init_state(state, enc_valid_lens)\n","        batch_size = en_input.shape[0]\n","        dec_input = torch.tensor([cfg.START]*batch_size, device=en_input.device).reshape(-1,1)\n","        \n","        pred_len = 0\n","        for _ in range(cfg.seq_len):\n","            # dec_input: (batch_size, 1)\n","            dec_input, hidden = self.decoder(dec_input, hidden)\n","            # dec_input: (batch_size, 1, vocab_size)\n","            dec_input = torch.argmax(dec_input, dim=2)\n","            # dec_input: (batch_size, 1)\n","            outputs.append(dec_input)\n","            if self.with_attn == True:\n","                # decoder.attn_weights: (batch_size, 1, num_k)\n","                attn_weights.append(self.decoder.attn_weights)\n","            if self.is_transformer is True:\n","                attn_weights[1].append(self.decoder.attn_weights[0])\n","                attn_weights[2].append(self.decoder.attn_weights[1])\n","            pred_len += 1\n","            if dec_input == cfg.END:\n","                break\n","        \n","        if self.with_attn == True:\n","            attn_weights = torch.concat(attn_weights, dim=1)\n","            # attn_weights: (batch_size, pred_len, num_k)\n","            # print(f'attn_weights: {attn_weights.shape}')\n","\n","        if self.is_transformer is True:\n","            attn_weights[1] = [head[-1].tolist() \n","                                for step in attn_weights[1]\n","                                for block in step\n","                                for head in block]\n","            attn_weights[1] = torch.tensor(pd.DataFrame(attn_weights[1]).fillna(0.0).values).reshape(-1, cfg.num_layers, cfg.num_heads, pred_len)\n","            attn_weights[1] = attn_weights[1].permute(1, 2, 0, 3)\n","            # attn_weights[1]: (num_layers, num_heads, pred_len, pred_len)\n","            attn_weights[2] = [head[-1].tolist() \n","                                for step in attn_weights[2]\n","                                for block in step\n","                                for head in block]\n","            attn_weights[2] = torch.tensor(pd.DataFrame(attn_weights[2]).fillna(0.0).values).reshape(-1, cfg.num_layers, cfg.num_heads, cfg.seq_len)\n","            attn_weights[2] = attn_weights[2].permute(1, 2, 0, 3)\n","            # attn_weights[2]: (num_layers, num_heads, pred_len, num_k)\n","            # print(f'attn_weights[0]: {attn_weights[0].shape}, attn_weights[1]: {attn_weights[1].shape}, attn_weights[2]: {attn_weights[2].shape}')\n","\n","        outputs = torch.concat(outputs, dim=1)\n","        return outputs, attn_weights\n","        \n","        \n","    def test(self, show_case=3):\n","        self.encoder.load_state_dict(torch.load(cfg.enc_model_path + '-' + self.save_path + '.pth'))\n","        self.decoder.load_state_dict(torch.load(cfg.dec_model_path + '-' + self.save_path + '.pth'))\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        \n","        avg_bleu = [0, 0, 0, 0]\n","        num_case = 0\n","        \n","        for i, (x, y) in enumerate(cfg.test_dataloader):\n","            if show_case != 0 and i == show_case:\n","                break\n","            \n","            x, y = x.to(device), y.to(device)\n","            x_valid_len = torch.sum(x!=cfg.PAD, dim=-1)\n","            output, attn_weights = self.predict(x, x_valid_len)\n","            \n","            y_valid_len = torch.sum(y!=cfg.PAD, dim=-1)\n","            x, y, output = x[0], y[0], output[0]\n","            x, y = x[:x_valid_len], y[:y_valid_len]\n","            \n","            pred_len = len(output)\n","\n","            origin_x = ' '.join([cfg.eng_vocab['idx_to_token'][_] for _ in x])\n","            origin_y = ''.join([cfg.cmn_vocab['idx_to_token'][_] for _ in y])\n","            pred = ''.join([cfg.cmn_vocab['idx_to_token'][_] for _ in output])\n","    \n","            origin_y = (' '.join(origin_y)).replace('< U N K >', '<UNK>').replace('< E N D >', '<END>')\n","            pred = ' '.join(pred).replace('< U N K >', '<UNK>').replace('< E N D >', '<END>')\n","    \n","            bleu_score = [0, 0, 0, 0]\n","            for i in range(4):\n","                bleu_score[i] = bleu(pred,origin_y,k=i+1)\n","                avg_bleu[i] += bleu_score[i]\n","            num_case += 1\n","            \n","            if num_case <= show_case:\n","                print(f\"origin [eng]: {origin_x}\")\n","                print(f\"origin [cmn]: {origin_y}\") \n","                print(f\"predict[cmn]: {pred}\")\n","                print(f\"BLEU: (1){bleu_score[0]} (2){bleu_score[1]} (3){bleu_score[2]} (4){bleu_score[3]}\\n\")\n","                \n","                if self.with_attn is True:\n","                    attn_weights = attn_weights.cpu().detach().numpy()\n","                    attn_df = pd.DataFrame(attn_weights[0][:, :x_valid_len[0]])\n","                    ax = sns.heatmap(attn_df, annot=True,  cmap=\"YlGnBu\")\n","                    plt.show()\n","                    \n","                if self.is_transformer is True:\n","                    attn_weights = [attn.cpu().detach().numpy() for attn in attn_weights]\n","\n","                    attn_enc = attn_weights[0][:, :, :x_valid_len[0], :x_valid_len[0]]\n","                    show_heatmaps(attn_enc, xlabel=\"Key positions\", ylabel=\"Query positions\", \n","                                titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Encoder Attention Weights',\n","                                figsize=(10,5), cmap=\"YlGnBu\")\n","                    attn_dec_self = attn_weights[1][:, :, :pred_len, :pred_len]\n","                    show_heatmaps(attn_dec_self, xlabel=\"Key positions\", ylabel=\"Query positions\",\n","                                titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Decoder Self Attention Weights',\n","                                figsize=(10,5), cmap=\"YlGnBu\")\n","                    attn_dec_inter = attn_weights[2][:, :, :pred_len, :x_valid_len[0]]\n","                    show_heatmaps(attn_dec_inter, xlabel=\"Key positions\", ylabel=\"Query positions\",\n","                                titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Decoder Inter Attention Weights',\n","                                figsize=(10,5), cmap=\"YlGnBu\")\n","        \n","        for i in range(4): \n","            avg_bleu[i] /= num_case\n","        print(f'Test for {num_case} cases, BLEU: (1){avg_bleu[0]} (2){avg_bleu[1]} (3){avg_bleu[2]} (4){avg_bleu[3]}')\n","        \n","        return avg_bleu\n","    \n","\n","    def online_predict(self, en_input, with_attn=False, is_transformer=False):\n","        self.encoder.load_state_dict(torch.load(cfg.enc_model_path + '-' + self.save_path + '.pth'))\n","        self.decoder.load_state_dict(torch.load(cfg.dec_model_path + '-' + self.save_path + '.pth'))\n","        self.encoder.eval()\n","        self.decoder.eval()\n","        \n","        # process input\n","        en_input = eng_transform(en_input)\n","\n","        print(f'Input: {en_input}')\n","\n","        en_input = line_to_idx(en_input.split(), lang='eng')\n","        en_input = torch.tensor(en_input, dtype=torch.long).reshape(1, -1).to(device)\n","        input_len = torch.sum(en_input!=cfg.PAD, dim=-1)\n","\n","        output, attn_weights = self.predict(en_input, input_len)\n","        \n","        output = output[0]\n","        pred_len = len(output)\n","        pred = ''.join([cfg.cmn_vocab['idx_to_token'][_] for _ in output])\n","        pred = ' '.join(pred).replace('< U N K >', '<UNK>').replace('< E N D >', '<END>')\n","        print(f'Pred: {pred}')\n","\n","        if with_attn is True:\n","            attn_weights = attn_weights.cpu().detach().numpy()\n","            attn_df = pd.DataFrame(attn_weights[0][:, :input_len])\n","            ax = sns.heatmap(attn_df, annot=True,  cmap=\"YlGnBu\")\n","            plt.show()\n","\n","        if is_transformer is True:\n","            attn_weights = [attn.cpu().detach().numpy() for attn in attn_weights]\n","\n","            attn_enc = attn_weights[0][:, :, :input_len, :input_len]\n","            show_heatmaps(attn_enc, xlabel=\"Key positions\", ylabel=\"Query positions\", \n","                        titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Encoder Attention Weights',\n","                        figsize=(10,5), cmap=\"YlGnBu\")\n","            attn_dec_self = attn_weights[1][:, :, :pred_len, :pred_len]\n","            show_heatmaps(attn_dec_self, xlabel=\"Key positions\", ylabel=\"Query positions\",\n","                        titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Decoder Self Attention Weights',\n","                        figsize=(10,5), cmap=\"YlGnBu\")\n","            attn_dec_inter = attn_weights[2][:, :, :pred_len, :input_len]\n","            show_heatmaps(attn_dec_inter, xlabel=\"Key positions\", ylabel=\"Query positions\",\n","                        titles=['Head %d' % i for i in range(1,5)], suptitle='Transformer Decoder Inter Attention Weights',\n","                        figsize=(10,5), cmap=\"YlGnBu\")                       "]},{"cell_type":"markdown","metadata":{},"source":["#### Training test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.099001Z","iopub.status.busy":"2022-09-06T09:17:36.096429Z","iopub.status.idle":"2022-09-06T09:17:36.108215Z","shell.execute_reply":"2022-09-06T09:17:36.107147Z","shell.execute_reply.started":"2022-09-06T09:17:36.098959Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# baseline_encoder = Seq2SeqEncoder(\n","#     vocab_size=cfg.eng_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=cfg.hidden_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","#     withBidirectional=False,\n","# )\n","# baseline_decoder = Seq2SeqDecoder(\n","#     vocab_size=cfg.cmn_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=H_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","# )\n","# solver_baseline = Seq2SeqSolver(baseline_encoder, baseline_decoder, save_path='baseline')\n","# solver_baseline.train(mini_train=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.114663Z","iopub.status.busy":"2022-09-06T09:17:36.114030Z","iopub.status.idle":"2022-09-06T09:17:36.126599Z","shell.execute_reply":"2022-09-06T09:17:36.125515Z","shell.execute_reply.started":"2022-09-06T09:17:36.114625Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# bidRNN_encoder = Seq2SeqEncoder(\n","#     vocab_size=cfg.eng_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=H_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","#     withBidirectional=True,\n","# )\n","# bidRNN_decoder = Seq2SeqDecoder(\n","#     vocab_size=cfg.cmn_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=cfg.hidden_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","# )\n","# solver_bidRNN = Seq2SeqSolver(bidRNN_encoder, bidRNN_decoder, save_path='bidRNN')\n","# solver_bidRNN.train(mini_train=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 0x05加入Attention机制 - AdditiveAttention"]},{"cell_type":"markdown","metadata":{},"source":["#### AttnMaskedSoftmax"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.132576Z","iopub.status.busy":"2022-09-06T09:17:36.130709Z","iopub.status.idle":"2022-09-06T09:17:36.158897Z","shell.execute_reply":"2022-09-06T09:17:36.157458Z","shell.execute_reply.started":"2022-09-06T09:17:36.131655Z"},"trusted":true},"outputs":[],"source":["def sequence_mask(target, valid_len, value=0):\n","    '''\n","    target: (batch_size, seq_len)\n","    valid_len: (batch_size,)\n","    '''\n","    bool_mask = torch.arange((target.shape[1]), dtype=torch.float32, device=target.device)[None, :] < valid_len[:, None]\n","    target[~bool_mask] = value\n","    return target\n","\n","target = torch.randint(1, 10, (4, 10))\n","valid_len = torch.tensor([10, 7, 5, 3])\n","mask_target = sequence_mask(target, valid_len)\n","mask_target"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.172748Z","iopub.status.busy":"2022-09-06T09:17:36.162412Z","iopub.status.idle":"2022-09-06T09:17:36.196797Z","shell.execute_reply":"2022-09-06T09:17:36.195810Z","shell.execute_reply.started":"2022-09-06T09:17:36.172706Z"},"trusted":true},"outputs":[],"source":["\n","def attn_masked_softmax(X, valid_len):\n","    \"\"\"\n","    对num_k这一维作mask 【num_k在实际运行中等于seq_len】\n","    X: (batch_size, num_q, num_k)\n","    valid_len: (batch_size,) or (num_heads, num_q)\n","    \"\"\"\n","    if valid_len is None:\n","        return nn.functional.softmax(X, dim=-1)\n","    else:\n","        shape = X.shape\n","        if valid_len.dim() == 1:\n","            valid_len = torch.repeat_interleave(valid_len, shape[1])\n","        else:\n","            valid_len = valid_len.reshape(-1)\n","        # 对被掩蔽的元素使用极小负值替换，从而其softmax输出为0\n","        X = sequence_mask(X.reshape(-1, shape[-1]), valid_len, value=-1e10)\n","        return nn.functional.softmax(X.reshape(shape), dim=-1)\n","    \n","x = torch.randn((2,4,10))\n","valid_len = torch.tensor([6,3])\n","attn_weight = attn_masked_softmax(x, valid_len)\n","print(attn_weight)"]},{"cell_type":"markdown","metadata":{},"source":["#### AdditiveAttention\n","$$\n","a(\\mathbf{q}, \\mathbf{k})=\\mathbf{W}_{v} \\tanh \\left(\\mathbf{W}_{q} \\mathbf{q}+\\mathbf{W}_{k} \\mathbf{k}\\right) \\\\\n","Attention(\\mathbf{q}) = \\sum_{i=1}^m softmax(a(\\mathbf{q}, \\mathbf{k_i})) \\mathbf{v_i}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.203605Z","iopub.status.busy":"2022-09-06T09:17:36.200228Z","iopub.status.idle":"2022-09-06T09:17:36.259579Z","shell.execute_reply":"2022-09-06T09:17:36.258508Z","shell.execute_reply.started":"2022-09-06T09:17:36.203568Z"},"trusted":true},"outputs":[],"source":["\n","class AdditiveAttention(nn.Module):\n","    ''' 加性注意力 '''\n","    def __init__(self, k_size, q_size, hidden_size, dropout=0.1):\n","        super(AdditiveAttention, self).__init__()\n","        self.w_q = nn.Linear(q_size, hidden_size, bias=False)\n","        self.w_k = nn.Linear(k_size, hidden_size, bias=False)\n","        self.w_v = nn.Linear(hidden_size, 1, bias=False)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","        self.attn_weight = None\n","        \n","    def forward(self, Q, K, V, valid_len):\n","        '''\n","        Q: (batch_size, num_q, q_size)\n","        K: (batch_size, num_k, k_size)\n","        V: (batch_size, num_k, hidden_size)\n","        '''\n","        Q, K = self.w_q(Q), self.w_k(K)\n","        # 扩展维度, 并使用广播方式求和\n","        # Q: (batch_size, num_q, 1, hidden_size)\n","        # K: (batch_size, 1, num_k, hidden_size)\n","        features = Q.unsqueeze(2) + K.unsqueeze(1)\n","        features = torch.tanh(features)\n","        # features: (batch_size, num_q, num_k, hidden_size)\n","        scores = self.w_v(features).squeeze(-1)\n","        # scores: (batch_size, num_q, num_k)\n","        attn_weight = attn_masked_softmax(scores, valid_len)\n","        # attn_weight: (batch_size, num_q, num_k)\n","        output = torch.bmm(self.dropout(attn_weight), V)\n","        # output: (batch_size, num_q, hidden_size)\n","        self.attn_weight = attn_weight\n","        return output\n","    \n","\n","test_attn = AdditiveAttention(k_size=128, q_size=128, hidden_size=128)\n","Q = torch.ones((64, 1, 128))\n","K = torch.ones((64, 10, 128))\n","V = torch.ones((64, 10, 128))\n","valid_len = torch.randint(2,10,(64,))\n","output = test_attn(Q,K,V,valid_len)\n","output.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Seq2SeqAttentionDecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:36.264399Z","iopub.status.busy":"2022-09-06T09:17:36.264027Z","iopub.status.idle":"2022-09-06T09:17:37.183250Z","shell.execute_reply":"2022-09-06T09:17:37.182307Z","shell.execute_reply.started":"2022-09-06T09:17:36.264365Z"},"trusted":true},"outputs":[],"source":["\n","class Seq2SeqAttentionDecoder(nn.Module):\n","    ''' Attention-解码器 '''\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.1):\n","        super(Seq2SeqAttentionDecoder, self).__init__()\n","        # ***\n","        self.attention = AdditiveAttention(k_size=hidden_size, q_size=hidden_size, hidden_size=hidden_size, dropout=dropout)\n","        # ***\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        # 加入注意力机制的上下文变量attn_context在所有的时间步与解码器的输入dec_input进行拼接（concatenate）。\n","        self.rnn = nn.GRU(embed_size+hidden_size, hidden_size, num_layers, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","        \n","        self.attn_weights = None\n","        \n","    def init_state(self, enc_outputs, enc_valid_len):\n","        enc_output, hidden = enc_outputs\n","        # enc_output: (seq_len, batch_size, hidden_size)\n","        # hidden: (num_layers, batch_size, hidden_size)\n","        enc_output = enc_output.permute(1, 0, 2) \n","        # enc_output: (batch_size, seq_len, hidden_size)\n","        return (enc_output, hidden, enc_valid_len)\n","    \n","    def forward(self, x, state):\n","        '''\n","        x: (batch_size, seq_len)\n","        state: tuple_like(init_state())\n","        '''\n","        enc_output, hidden, enc_valid_len = state\n","        K, V = enc_output, enc_output\n","        x = self.embedding(x).permute(1, 0, 2) # (seq_len, batch_size, embed_size)\n","        seq_len = x.shape[0]\n","        \n","        outputs, attn_weights = [], []\n","        \n","        for i in range(seq_len):\n","            Q = hidden[-1]                  # (batch_size, hidden_size)\n","            Q = torch.unsqueeze(Q, dim=1)  # (batch_size, 1, hidden_size)\n","            attn_context = self.attention(Q, K, V, enc_valid_len)   # (batch_size, 1, hidden_size)\n","            x_context = torch.cat((torch.unsqueeze(x[i], dim=1), attn_context), dim=-1).permute(1, 0, 2)\n","            # x_context: (1, batch_size, embed_size+hidden_size)\n","            dec_output, hidden = self.rnn(x_context, hidden)\n","            # dec_output: (1, batch_size, hidden_size)\n","            # hidden: (num_layers, batch_size, hidden_size)\n","            outputs.append(dec_output)\n","            attn_weights.append(self.attention.attn_weight)\n","            # attn_weights: (batch_size, 1, num_k)\n","        \n","        self.attn_weights = torch.concat(attn_weights, dim=1)\n","        # self.attn_weights: (batch_size, x_seq_len, num_k)\n","        outputs = torch.cat(outputs, dim=0)\n","        outputs = self.fc(outputs).permute(1, 0, 2)\n","        # outputs: (batch_size, seq_len, vocab_size)\n","        state = (enc_output, hidden, enc_valid_len)\n","        return outputs, state\n","        \n","    \n","attn_decoder_tester = Seq2SeqAttentionDecoder(vocab_size=1024, embed_size=128, hidden_size=128, num_layers=2, dropout=0.1)\n","x_test = torch.randint(1, 100, (64, 10))\n","x_valid_len = torch.randint(2, 10, (64, ))\n","\n","enc_outputs = encoder_tester(x_test)\n","state = attn_decoder_tester.init_state(enc_outputs, x_valid_len)\n","\n","output, state = attn_decoder_tester(x_test, state)\n","output.shape, len(state), state[0].shape, state[1].shape, state[2].shape\n","# (batch_size, seq_len, vocab_size), 3, (batch_size, seq_len, hidden_size), (num_layers, batch_size, hidden_size), (batch_size, )\n","\n","# show attn_weights\n","attn_weights = attn_decoder_tester.attn_weights.detach().numpy()\n","print(attn_weights[0].shape)\n","\n","attn_df = pd.DataFrame(attn_weights[0][:, :x_valid_len[0]])\n","\n","ax = sns.heatmap(attn_df, annot=True,  cmap=\"YlGnBu\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### Training test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.185736Z","iopub.status.busy":"2022-09-06T09:17:37.184578Z","iopub.status.idle":"2022-09-06T09:17:37.191476Z","shell.execute_reply":"2022-09-06T09:17:37.190550Z","shell.execute_reply.started":"2022-09-06T09:17:37.185698Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# attention_encoder = Seq2SeqEncoder(\n","#     vocab_size=cfg.eng_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=cfg.hidden_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","#     withBidirectional=False,\n","# )\n","# attention_decoder = Seq2SeqAttentionDecoder(\n","#     vocab_size=cfg.cmn_vocab_size,\n","#     embed_size=cfg.embed_size,\n","#     hidden_size=cfg.hidden_size,\n","#     num_layers=cfg.num_layers,\n","#     dropout=cfg.dropout,\n","# )\n","# solver_attention = Seq2SeqSolver(attention_encoder, attention_decoder, with_attn=True, save_path='attention')\n","# solver_attention.train(mini_train=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 0x06Transformer"]},{"cell_type":"markdown","metadata":{},"source":["#### Scaled Dot-Product Attention\n","$$\n","Attention(Q,K,V) = \\text{dropout}\\bigg(\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right)\\bigg) \\mathbf{V}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.194410Z","iopub.status.busy":"2022-09-06T09:17:37.193668Z","iopub.status.idle":"2022-09-06T09:17:37.218857Z","shell.execute_reply":"2022-09-06T09:17:37.217540Z","shell.execute_reply.started":"2022-09-06T09:17:37.194373Z"},"trusted":true},"outputs":[],"source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, dropout) -> None:\n","        super(ScaledDotProductAttention, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, Q, K, V, valid_lens=None):\n","        '''\n","        Q = (batch_size, num_query, query_size[d])\n","        K = (batch_size, num_key, key_size[d])\n","        V = (batch_size, num_key, value_size[v])\n","        valid_lens = None or (batch_size, ) or (batch_size, num_query)\n","        '''\n","        d = Q.shape[-1]\n","        scores = torch.bmm(Q, K.transpose(-2, -1)) / np.sqrt(d)\n","        # scores = (batch_size, num_query, num_key)\n","        self.attn_weights = attn_masked_softmax(scores, valid_lens)\n","        # self.attn_weights = (batch_size, num_query, num_key)\n","        out = torch.bmm(self.dropout(self.attn_weights), V)\n","        # out = (batch_size, num_query, value_size[v])\n","        return out\n","\n","queries = torch.normal(0, 1, (2, 1, 2))\n","test_attn = ScaledDotProductAttention(dropout=0.5)\n","Q = torch.ones((64, 10, 128))\n","K = torch.ones((64, 10, 128))\n","V = torch.ones((64, 10, 128))\n","valid_len = torch.randint(2,10,(64,))\n","test_attn.eval()\n","output = test_attn(Q,K,V,valid_len)\n","output.shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Multi-Head Attention\n","$$\n","Y_i = \\text{dropout}\\bigg(\\text{softmax}\\bigg(\\frac{(XQ_i)(XK_i)^\\top}{\\sqrt{d/h}}\\bigg)\\bigg)(XV_i) \\\\\n","Y = [Y_1;\\dots;Y_h]W_o\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.222723Z","iopub.status.busy":"2022-09-06T09:17:37.222302Z","iopub.status.idle":"2022-09-06T09:17:37.255268Z","shell.execute_reply":"2022-09-06T09:17:37.254263Z","shell.execute_reply.started":"2022-09-06T09:17:37.222674Z"},"trusted":true},"outputs":[],"source":["\n","def transpose_QKV(x, num_heads):\n","    '''\n","    x = (batch_size, num_query or num_key, hidden_size)\n","    '''\n","    x = x.reshape(x.shape[0], x.shape[1], num_heads, -1)\n","    x = x.permute(0, 2, 1, 3)\n","    # x = (batch_size, num_heads, num_query or num_key, hidden_size/num_heads)\n","    out = x.reshape(-1, x.shape[2], x.shape[3])\n","    # out = (batch_size * num_heads, num_query or num_key, hidden_size/num_heads)\n","    return out\n","\n","def transpose_QKV_inv(x, num_heads):\n","    '''\n","    x = (batch_size * num_heads, num_query or num_key, hidden_size/num_heads)\n","    '''\n","    x = x.reshape(-1, num_heads, x.shape[1], x.shape[2])\n","    x = x.permute(0, 2, 1, 3)\n","    # x = (batch_size, num_query or num_key, num_heads, hidden_size/num_heads)\n","    out = x.reshape(x.shape[0], x.shape[1], -1)\n","    # out = (batch_size, num_query or num_key, hidden_size)\n","    return out\n","\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, query_size, key_size, value_size, hidden_size, num_heads, dropout, bias=False) -> None:\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.attention = ScaledDotProductAttention(dropout=dropout)\n","        # Projects the query, key and value.\n","        self.W_q = nn.Linear(query_size, hidden_size, bias=bias)\n","        self.W_k = nn.Linear(key_size, hidden_size, bias=bias)\n","        self.W_v = nn.Linear(value_size, hidden_size, bias=bias)\n","        self.W_o = nn.Linear(hidden_size, hidden_size, bias=bias)\n","\n","    def forward(self, Q, K, V, valid_lens=None):\n","        '''\n","        Q = (batch_size, num_query, query_size[d])\n","        K = (batch_size, num_key, key_size[d])\n","        V = (batch_size, num_key, value_size[v])\n","        valid_lens = None or (batch_size, ) or (batch_size, num_query)\n","        '''\n","        Q = transpose_QKV(self.W_q(Q), self.num_heads)\n","        K = transpose_QKV(self.W_k(K), self.num_heads)\n","        V = transpose_QKV(self.W_v(V), self.num_heads)\n","        \n","        if valid_lens is not None:\n","            if valid_lens.dim() == 1:\n","                valid_lens = valid_lens.repeat(self.num_heads, 1).reshape(-1)\n","            else:\n","                valid_lens = valid_lens.repeat(self.num_heads, 1)\n","\n","        # Multi-Head Attention\n","        out = self.attention(Q, K, V, valid_lens)\n","        # Concat\n","        out_concat = transpose_QKV_inv(out, self.num_heads)\n","        # output\n","        out = self.W_o(out_concat)\n","        # out = (batch_size, num_query, hidden_size)\n","        return out\n","\n","hidden_size, num_heads = 128, 4\n","test_attn = MultiHeadAttention(query_size=hidden_size, key_size=hidden_size, value_size=hidden_size, hidden_size=hidden_size, num_heads=num_heads, dropout=0.5)\n","test_attn.eval()\n","Q = torch.ones((64, 1, hidden_size))\n","K = torch.ones((64, 10, hidden_size))\n","V = torch.ones((64, 10, hidden_size))\n","valid_len = torch.randint(2,10,(64,))\n","test_attn(Q, K, V, valid_len).shape\n"]},{"cell_type":"markdown","metadata":{},"source":["#### PositionWiseFFN\n","$$\n","\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.257309Z","iopub.status.busy":"2022-09-06T09:17:37.256748Z","iopub.status.idle":"2022-09-06T09:17:37.264020Z","shell.execute_reply":"2022-09-06T09:17:37.262858Z","shell.execute_reply.started":"2022-09-06T09:17:37.257272Z"},"trusted":true},"outputs":[],"source":["class PositionWiseFFN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size) -> None:\n","        super(PositionWiseFFN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        return self.fc2(self.relu(self.fc1(x)))"]},{"cell_type":"markdown","metadata":{},"source":["#### AddNorm"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.266263Z","iopub.status.busy":"2022-09-06T09:17:37.265802Z","iopub.status.idle":"2022-09-06T09:17:37.283267Z","shell.execute_reply":"2022-09-06T09:17:37.280936Z","shell.execute_reply.started":"2022-09-06T09:17:37.266112Z"},"trusted":true},"outputs":[],"source":["class AddNorm(nn.Module):\n","    '''\n","    Residual connection + Layer Normalization\n","    '''\n","    def __init__(self, normalized_shape, dropout) -> None:\n","        super(AddNorm, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","        self.ln = nn.LayerNorm(normalized_shape=normalized_shape)\n","\n","    def forward(self, x, y):\n","        '''\n","        x.shape = y.shape = (batch_size, seq_len, hidden_size)\n","        '''\n","        return self.ln(self.dropout(y) + x)\n","\n","test_addnorm = AddNorm(normalized_shape=128, dropout=0.5)\n","test_addnorm.eval()\n","x = torch.ones((64, 1, 128))\n","y = torch.ones((64, 1, 128))\n","test_addnorm(x, y).shape"]},{"cell_type":"markdown","metadata":{},"source":["#### PositionalEncoding\n","$$\n","\\begin{aligned}\n","P E_{(p o s, 2 i)} &=\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\\n","P E_{(p o s, 2 i+1)} &=\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right)\n","\\end{aligned}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.287427Z","iopub.status.busy":"2022-09-06T09:17:37.287127Z","iopub.status.idle":"2022-09-06T09:17:37.314886Z","shell.execute_reply":"2022-09-06T09:17:37.313933Z","shell.execute_reply.started":"2022-09-06T09:17:37.287393Z"},"trusted":true},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, embed_dim, dropout=0.1, max_len=100) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        assert embed_dim % 2 == 0, \"Embedding dimension must be even.\"\n","        pe = torch.zeros(1, max_len, embed_dim)\n","        val1 = [pos / (10000 ** (i / embed_dim)) for pos in range(max_len) for i in range(0, embed_dim, 2)]\n","        val2 = [pos / (10000 ** ( (i - 1) / embed_dim)) for pos in range(max_len) for i in range(1, embed_dim, 2)]\n","        pe[0, :, 0::2] = torch.sin(torch.tensor(val1)).reshape(max_len, -1)\n","        pe[0, :, 1::2] = torch.cos(torch.tensor(val2)).reshape(max_len, -1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        '''\n","        x = (batch_size, seq_len, embed_dim)\n","        '''\n","        x = x + self.pe[:, :x.shape[1], :x.shape[2]]\n","        return self.dropout(x)\n","\n","torch.manual_seed(231)\n","x = torch.randn(1, 2, 6)\n","\n","test_pe = PositionalEncoding(embed_dim=6, dropout=0.1)\n","# test_pe.eval() # eval() will disable dropout\n","output = test_pe(x)\n","print(output)\n","\n","expected_pe_output = np.asarray([[[-1.2340,  1.1127,  1.6978, -0.0865, -0.0000,  1.2728],\n","                                  [ 0.9028, -0.4781,  0.5535,  0.8133,  1.2644,  1.7034]]])\n","\n","def rel_error(x, y):\n","    \"\"\" returns relative error \"\"\"\n","    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n","\n","print('pe_output error: ', rel_error(expected_pe_output, output.detach().numpy()))\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.325366Z","iopub.status.busy":"2022-09-06T09:17:37.324932Z","iopub.status.idle":"2022-09-06T09:17:37.633753Z","shell.execute_reply":"2022-09-06T09:17:37.632845Z","shell.execute_reply.started":"2022-09-06T09:17:37.325325Z"},"trusted":true},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    def __init__(self, Q_size, K_size, V_size, hidden_size, norm_shape,\n","                ffn_input_size, ffn_hidden_size, num_heads, dropout, use_bias=False) -> None:\n","        super(EncoderBlock, self).__init__()\n","        self.attention = MultiHeadAttention(query_size=Q_size, key_size=K_size, value_size=V_size, hidden_size=hidden_size, num_heads=num_heads, dropout=dropout, bias=use_bias)\n","        self.addnorm1 = AddNorm(normalized_shape=norm_shape, dropout=dropout)\n","        self.ffn = PositionWiseFFN(input_size=ffn_input_size, hidden_size=ffn_hidden_size, output_size=hidden_size)\n","        self.addnorm2 = AddNorm(normalized_shape=norm_shape, dropout=dropout)\n","\n","    def forward(self, x, valid_lens):\n","        '''\n","        x = (batch_size, seq_len, hidden_size)\n","        valid_lens = (batch_size, )\n","        '''\n","        # self Multi-Head Attention + AddNorm\n","        out = self.addnorm1(x, self.attention(x, x, x, valid_lens))\n","        # Position-wise Feed-Forward Network + AddNorm\n","        out = self.addnorm2(out, self.ffn(out))\n","        return out\n","\n","hidden_size, num_heads = 1024, 2\n","x = torch.ones((64, 10, hidden_size))\n","test_enc_block = \\\n","    EncoderBlock(Q_size=hidden_size, K_size=hidden_size, V_size=hidden_size, hidden_size=hidden_size, norm_shape=[10, hidden_size],\n","                 ffn_input_size=hidden_size, ffn_hidden_size=hidden_size*2, num_heads=num_heads, dropout=0.5)\n","test_enc_block.eval()\n","valid_len = torch.randint(2,10,(64,))\n","test_enc_block(x, valid_len).shape\n","# 输入输出维度不变"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.639850Z","iopub.status.busy":"2022-09-06T09:17:37.637745Z","iopub.status.idle":"2022-09-06T09:17:37.697856Z","shell.execute_reply":"2022-09-06T09:17:37.696726Z","shell.execute_reply.started":"2022-09-06T09:17:37.639811Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, vocab_size, num_layers, Q_size, K_size, V_size, hidden_size, norm_shape,\n","                ffn_input_size, ffn_hidden_size, num_heads, dropout, use_bias=False) -> None:\n","        super(TransformerEncoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n","        self.encoder_blocks = nn.Sequential()\n","        self.num_layers = num_layers\n","        for i in range(num_layers):\n","            self.encoder_blocks.add_module(\n","                f'encoder_block_{i}', \n","                EncoderBlock(Q_size=Q_size, K_size=K_size, V_size=V_size, hidden_size=hidden_size, \n","                            norm_shape=norm_shape, ffn_input_size=ffn_input_size, ffn_hidden_size=ffn_hidden_size,\n","                            num_heads=num_heads, dropout=dropout, use_bias=use_bias))\n","\n","    def forward(self, x, valid_lens):\n","        '''\n","        x = (batch_size, seq_len)\n","        valid_lens = (batch_size, )\n","        '''\n","        # scale embedding\n","        x = self.embedding(x) * math.sqrt(self.hidden_size)\n","        # Positional Encoding\n","        x = self.pos_encoding(x)\n","        self.attn_weights = [None] * self.num_layers\n","        # encoder block\n","        for i, block in enumerate(self.encoder_blocks):\n","            x = block(x, valid_lens)\n","            self.attn_weights[i] = block.attention.attention.attn_weights\n","\n","        return x\n","\n","vocab_size, num_layers = 200, 2\n","hidden_size = 128\n","x = torch.randint(0, vocab_size, (64, 10))\n","print(x.shape)\n","test_enc = TransformerEncoder(vocab_size=vocab_size, num_layers=num_layers, Q_size=hidden_size, K_size=hidden_size, V_size=hidden_size, hidden_size=hidden_size, norm_shape=[10, hidden_size],\n","                            ffn_input_size=hidden_size, ffn_hidden_size=hidden_size*2, num_heads=8, dropout=0.5)\n","test_enc.eval()\n","valid_len = torch.randint(2,10,(64,))\n","test_enc(x, valid_len).shape"]},{"cell_type":"markdown","metadata":{},"source":["#### Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.702132Z","iopub.status.busy":"2022-09-06T09:17:37.701757Z","iopub.status.idle":"2022-09-06T09:17:37.758252Z","shell.execute_reply":"2022-09-06T09:17:37.757186Z","shell.execute_reply.started":"2022-09-06T09:17:37.702094Z"},"trusted":true},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","    def __init__(self, Q_size, K_size, V_size, hidden_size, norm_shape,\n","                ffn_input_size, ffn_hidden_size, num_heads, dropout, block_idx) -> None:\n","        super(DecoderBlock, self).__init__()\n","        self.block_idx = block_idx\n","        self.attention1 = MultiHeadAttention(query_size=Q_size, key_size=K_size, value_size=V_size, hidden_size=hidden_size, num_heads=num_heads, dropout=dropout)\n","        self.addnorm1 = AddNorm(normalized_shape=norm_shape, dropout=dropout)\n","        self.attention2 = MultiHeadAttention(query_size=Q_size, key_size=K_size, value_size=V_size, hidden_size=hidden_size, num_heads=num_heads, dropout=dropout)\n","        self.addnorm2 = AddNorm(normalized_shape=norm_shape, dropout=dropout)\n","        self.ffn = PositionWiseFFN(input_size=ffn_input_size, hidden_size=ffn_hidden_size, output_size=hidden_size)\n","        self.addnorm3 = AddNorm(normalized_shape=norm_shape, dropout=dropout)\n","\n","\n","    def forward(self, x, state):\n","        '''\n","        x = (batch_size, seq_len, hidden_size)\n","        state = tuple( (batch_size, seq_len, hidden_size), (batch_size, ) )\n","        '''\n","        if self.training:\n","            dec_valid_lens = torch.arange(1, x.shape[1]+1, device=x.device).repeat(x.shape[0], 1)\n","        else:\n","            dec_valid_lens = None\n","\n","        # self Multi-Head Attention + add & norm\n","        out = self.addnorm1(x, self.attention1(x, x, x, dec_valid_lens))\n","        # encoder-decoder Multi-Head Attention + add & norm\n","        enc_out, enc_valid_lens = state[0], state[1]\n","        out = self.addnorm2(out, self.attention2(out, enc_out, enc_out, enc_valid_lens))\n","        # Position-wise Feed-Forward Network + add & norm\n","        out = self.addnorm3(out, self.ffn(out))\n","        return out, state\n","\n","\n","hidden_size, num_heads = 128, 2\n","x = torch.ones((64, 10, hidden_size))\n","test_enc_block = \\\n","    EncoderBlock(Q_size=hidden_size, K_size=hidden_size, V_size=hidden_size, hidden_size=hidden_size, norm_shape=[10, hidden_size],\n","                 ffn_input_size=hidden_size, ffn_hidden_size=hidden_size*2, num_heads=num_heads, dropout=0.5)\n","test_dec_block = \\\n","    DecoderBlock(Q_size=hidden_size, K_size=hidden_size, V_size=hidden_size, hidden_size=hidden_size, norm_shape=[10, hidden_size],\n","                 ffn_input_size=hidden_size, ffn_hidden_size=hidden_size*2, num_heads=num_heads, dropout=0.5, block_idx=0)\n","# test_dec_block.eval() # self.training = False\n","\n","valid_len = torch.randint(2,10,(64,))\n","state = (test_enc_block(x, valid_len), valid_len)\n","output = test_dec_block(x, state)\n","output[0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.763795Z","iopub.status.busy":"2022-09-06T09:17:37.762375Z","iopub.status.idle":"2022-09-06T09:17:37.778944Z","shell.execute_reply":"2022-09-06T09:17:37.777822Z","shell.execute_reply.started":"2022-09-06T09:17:37.763733Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, num_layers, Q_size, K_size, V_size, hidden_size, norm_shape,\n","                ffn_input_size, ffn_hidden_size, num_heads, dropout) -> None:\n","        super(TransformerDecoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n","        self.decoder_blocks = nn.Sequential()\n","        for i in range(num_layers):\n","            self.decoder_blocks.add_module(\n","                f'decoder_block_{i}', \n","                DecoderBlock(Q_size=Q_size, K_size=K_size, V_size=V_size, hidden_size=hidden_size, norm_shape=norm_shape,\n","                            ffn_input_size=ffn_input_size, ffn_hidden_size=ffn_hidden_size, num_heads=num_heads, dropout=dropout, block_idx=i))\n","        self.fc = nn.Linear(hidden_size, vocab_size)\n","    \n","    def init_state(self, enc_out, enc_valid_lens):\n","        '''\n","        enc_out = (batch_size, seq_len, hidden_size)\n","        enc_valid_lens = (batch_size, )\n","        '''\n","        self.preX = None\n","        return enc_out, enc_valid_lens\n","\n","    def forward(self, x, state):\n","        '''\n","        x = (batch_size, seq_len)\n","        '''\n","        if not self.training:\n","            self.preX = x if self.preX is None else torch.cat((self.preX, x), dim=1)\n","            x = self.preX    \n","        \n","        # scale embedding\n","        x = self.embedding(x) * math.sqrt(self.hidden_size)\n","        # add position encoding\n","        x = self.pos_encoding(x)\n","        self.attn_weights = [[None] * self.num_layers for _ in range(2)]\n","        # decoder blocks\n","        for i, dec_block in enumerate(self.decoder_blocks):\n","            x, state = dec_block(x, state)\n","            # enc attention weights\n","            self.attn_weights[0][i] = dec_block.attention1.attention.attn_weights\n","            # enc-dec attention weights\n","            self.attn_weights[1][i] = dec_block.attention2.attention.attn_weights\n","        if not self.training:\n","            x = x[:, -1:, :]\n","        \n","        return self.fc(x), state\n","        "]},{"cell_type":"markdown","metadata":{},"source":["#### Training test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.781367Z","iopub.status.busy":"2022-09-06T09:17:37.780991Z","iopub.status.idle":"2022-09-06T09:17:37.791918Z","shell.execute_reply":"2022-09-06T09:17:37.790769Z","shell.execute_reply.started":"2022-09-06T09:17:37.781327Z"},"trusted":true},"outputs":[],"source":["# %%time\n","\n","# H_size = cfg.transformer_hidden_size\n","\n","# transformer_encoder = \\\n","#     TransformerEncoder(vocab_size=cfg.eng_vocab_size, num_layers=cfg.num_layers, \n","#                         Q_size=H_size, K_size=H_size, V_size=H_size, hidden_size=H_size, \n","#                         norm_shape=[H_size], ffn_input_size=H_size, ffn_hidden_size=H_size*2, \n","#                         num_heads=cfg.num_heads, dropout=0.1)\n","# transformer_decoder = \\\n","#     TransformerDecoder(vocab_size=cfg.cmn_vocab_size, num_layers=cfg.num_layers, \n","#                         Q_size=H_size, K_size=H_size, V_size=H_size, hidden_size=H_size, \n","#                         norm_shape=[H_size], ffn_input_size=H_size, ffn_hidden_size=H_size*2, \n","#                         num_heads=cfg.num_heads, dropout=0.1)\n","\n","# solver_transformer = Seq2SeqSolver(\n","#     transformer_encoder, transformer_decoder, \n","#     with_attn=True, is_transformer=True, save_path='transformer')\n","# solver_transformer.train(mini_train=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 0x07训练"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:17:37.838321Z","iopub.status.busy":"2022-09-06T09:17:37.837751Z","iopub.status.idle":"2022-09-06T09:23:19.267892Z","shell.execute_reply":"2022-09-06T09:23:19.266814Z","shell.execute_reply.started":"2022-09-06T09:17:37.838286Z"},"trusted":true},"outputs":[],"source":["%%time\n","st = time.time()\n","baseline_encoder = Seq2SeqEncoder(\n","    vocab_size=cfg.eng_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n","    withBidirectional=False,\n",")\n","baseline_decoder = Seq2SeqDecoder(\n","    vocab_size=cfg.cmn_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n",")\n","solver_baseline = Seq2SeqSolver(baseline_encoder, baseline_decoder, save_path='baseline')\n","solver_baseline.train()\n","time_baseline = time.time() - st"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:23:19.269827Z","iopub.status.busy":"2022-09-06T09:23:19.269354Z","iopub.status.idle":"2022-09-06T09:31:39.953594Z","shell.execute_reply":"2022-09-06T09:31:39.952542Z","shell.execute_reply.started":"2022-09-06T09:23:19.269790Z"},"trusted":true},"outputs":[],"source":["%%time\n","st = time.time()\n","bidRNN_encoder = Seq2SeqEncoder(\n","    vocab_size=cfg.eng_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n","    withBidirectional=True,\n",")\n","bidRNN_decoder = Seq2SeqDecoder(\n","    vocab_size=cfg.cmn_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n",")\n","solver_bidRNN = Seq2SeqSolver(bidRNN_encoder, bidRNN_decoder, save_path='bidRNN')\n","solver_bidRNN.train()\n","time_bidRNN = time.time() - st"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:31:39.956251Z","iopub.status.busy":"2022-09-06T09:31:39.955575Z","iopub.status.idle":"2022-09-06T09:42:33.881878Z","shell.execute_reply":"2022-09-06T09:42:33.879920Z","shell.execute_reply.started":"2022-09-06T09:31:39.956212Z"},"trusted":true},"outputs":[],"source":["%%time\n","st = time.time()\n","attention_encoder = Seq2SeqEncoder(\n","    vocab_size=cfg.eng_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n","    withBidirectional=False,\n",")\n","attention_decoder = Seq2SeqAttentionDecoder(\n","    vocab_size=cfg.cmn_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n",")\n","solver_attention = Seq2SeqSolver(attention_encoder, attention_decoder, with_attn=True, save_path='attention')\n","solver_attention.train()\n","time_attention = time.time() - st"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:42:33.884005Z","iopub.status.busy":"2022-09-06T09:42:33.883644Z","iopub.status.idle":"2022-09-06T09:56:22.858008Z","shell.execute_reply":"2022-09-06T09:56:22.856743Z","shell.execute_reply.started":"2022-09-06T09:42:33.883970Z"},"trusted":true},"outputs":[],"source":["%%time\n","st = time.time()\n","bidRNN_attention_encoder = Seq2SeqEncoder(\n","    vocab_size=cfg.eng_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n","    withBidirectional=True,\n",")\n","bidRNN_attention_decoder = Seq2SeqAttentionDecoder(\n","    vocab_size=cfg.cmn_vocab_size,\n","    embed_size=cfg.embed_size,\n","    hidden_size=cfg.hidden_size,\n","    num_layers=cfg.num_layers,\n","    dropout=cfg.dropout,\n",")\n","solver_bidRNN_attention = Seq2SeqSolver(bidRNN_attention_encoder, bidRNN_attention_decoder, with_attn=True, save_path='bidRNN_attention')\n","solver_bidRNN_attention.train()\n","time_bidRNN_attention = time.time() - st"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:56:22.859931Z","iopub.status.busy":"2022-09-06T09:56:22.859441Z","iopub.status.idle":"2022-09-06T09:59:54.917596Z","shell.execute_reply":"2022-09-06T09:59:54.916610Z","shell.execute_reply.started":"2022-09-06T09:56:22.859891Z"},"trusted":true},"outputs":[],"source":["%%time\n","st = time.time()\n","\n","H_size = cfg.transformer_hidden_size\n","\n","transformer_encoder = \\\n","    TransformerEncoder(vocab_size=cfg.eng_vocab_size, num_layers=cfg.num_layers, \n","                        Q_size=H_size, K_size=H_size, V_size=H_size, hidden_size=H_size, \n","                        norm_shape=[H_size], ffn_input_size=H_size, ffn_hidden_size=H_size*2, \n","                        num_heads=cfg.num_heads, dropout=0.1)\n","transformer_decoder = \\\n","    TransformerDecoder(vocab_size=cfg.cmn_vocab_size, num_layers=cfg.num_layers, \n","                        Q_size=H_size, K_size=H_size, V_size=H_size, hidden_size=H_size, \n","                        norm_shape=[H_size], ffn_input_size=H_size, ffn_hidden_size=H_size*2, \n","                        num_heads=cfg.num_heads, dropout=0.1)\n","\n","solver_transformer = Seq2SeqSolver(\n","    transformer_encoder, transformer_decoder, \n","    is_transformer=True, save_path='transformer')\n","solver_transformer.train()\n","time_transformer = time.time() - st"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:19:55.937510Z","iopub.status.busy":"2022-09-06T10:19:55.936407Z","iopub.status.idle":"2022-09-06T10:19:56.185297Z","shell.execute_reply":"2022-09-06T10:19:56.184387Z","shell.execute_reply.started":"2022-09-06T10:19:55.937380Z"},"trusted":true},"outputs":[],"source":["cfg.all_losslog['baseline'] = solver_baseline.losslog\n","cfg.all_losslog['bidRNN'] = solver_bidRNN.losslog\n","cfg.all_losslog['attention'] = solver_attention.losslog\n","cfg.all_losslog['bidRNN_attention'] = solver_bidRNN_attention.losslog\n","cfg.all_losslog['transformer'] = solver_transformer.losslog\n","\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.plot(np.arange(cfg.num_epoches), cfg.all_losslog['baseline'], label='baseline', linewidth =2.0, color='black')\n","plt.plot(np.arange(cfg.num_epoches), cfg.all_losslog['bidRNN'], label='bidRNN', linewidth =2.0, color='blue')\n","plt.plot(np.arange(cfg.num_epoches), cfg.all_losslog['attention'], label='attention', linewidth =2.0, color='orange')\n","plt.plot(np.arange(cfg.num_epoches), cfg.all_losslog['bidRNN_attention'], label='bidRNN_attention', linewidth =2.0, color='red')\n","plt.plot(np.arange(cfg.num_epoches), cfg.all_losslog['transformer'], label='transformer', linewidth =2.0, color='green')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 0x08测试"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T09:59:55.175173Z","iopub.status.busy":"2022-09-06T09:59:55.174845Z","iopub.status.idle":"2022-09-06T10:03:13.138471Z","shell.execute_reply":"2022-09-06T10:03:13.137356Z","shell.execute_reply.started":"2022-09-06T09:59:55.175139Z"},"trusted":true},"outputs":[],"source":["baseline_bleu = solver_baseline.test(show_case=0)\n","bidRNN_bleu = solver_bidRNN.test(show_case=0)\n","attention_bleu = solver_attention.test(show_case=0)\n","bidRNN_attention_bleu = solver_bidRNN_attention.test(show_case=0)\n","transformer_bleu = solver_transformer.test(show_case=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:15:39.967092Z","iopub.status.busy":"2022-09-06T10:15:39.966402Z","iopub.status.idle":"2022-09-06T10:15:39.986278Z","shell.execute_reply":"2022-09-06T10:15:39.985035Z","shell.execute_reply.started":"2022-09-06T10:15:39.967056Z"},"trusted":true},"outputs":[],"source":["all_metric = [baseline_bleu+[time_baseline], bidRNN_bleu+[time_bidRNN], \n","            attention_bleu+[time_attention], bidRNN_attention_bleu+[time_bidRNN_attention], \n","            transformer_bleu+[time_transformer]]\n","all_metric = np.array(all_metric)\n","cols = [\"1-BLEU\", \"2-BLEU\", \"3-BLEU\", \"4-BLEU\", \"Training Time (seconds)\"]\n","rows = ['baseline', 'bidRNN', 'attention', 'bidRNN_attention', 'transformer']\n","bleu_results = pd.DataFrame(data=all_metric, columns=cols, index=rows)\n","bleu_results.round(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:03:13.159691Z","iopub.status.busy":"2022-09-06T10:03:13.159193Z","iopub.status.idle":"2022-09-06T10:03:15.166435Z","shell.execute_reply":"2022-09-06T10:03:15.165465Z","shell.execute_reply.started":"2022-09-06T10:03:13.159649Z"},"trusted":true},"outputs":[],"source":["test_bleu = solver_bidRNN_attention.test(show_case=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:03:15.171645Z","iopub.status.busy":"2022-09-06T10:03:15.170697Z","iopub.status.idle":"2022-09-06T10:03:24.027276Z","shell.execute_reply":"2022-09-06T10:03:24.026307Z","shell.execute_reply.started":"2022-09-06T10:03:15.171608Z"},"trusted":true},"outputs":[],"source":["test_bleu = solver_transformer.test(show_case=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:28:33.480256Z","iopub.status.busy":"2022-09-06T10:28:33.479377Z","iopub.status.idle":"2022-09-06T10:28:34.146735Z","shell.execute_reply":"2022-09-06T10:28:34.145480Z","shell.execute_reply.started":"2022-09-06T10:28:33.480208Z"},"trusted":true},"outputs":[],"source":["solver_bidRNN_attention.online_predict(\"this is an interesting test .\", with_attn=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-06T10:28:44.897551Z","iopub.status.busy":"2022-09-06T10:28:44.896926Z","iopub.status.idle":"2022-09-06T10:28:47.886385Z","shell.execute_reply":"2022-09-06T10:28:47.885427Z","shell.execute_reply.started":"2022-09-06T10:28:44.897509Z"},"trusted":true},"outputs":[],"source":["solver_transformer.online_predict(\"this is an interesting test .\", is_transformer=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
